\section{Greedy Algorithm}

    A greedy algorithm always makes the choice that looks best at the moment.
    That is , it makes a locally optimal choice in the hope that this choice
    will lead to a globally optimal solution.

\subsection{An activity-selection problem}

    Suppose we have a set $S=\lbrace a_1,a_2,...,a_n \rbrace$
    of n proposed activities that wish to use a resource, 
    such as a lecture hall, which
    can serve only one activity at a time.

    Each activity $a_i$ has a start time $s_i$ and a finish time $f_i$,
    where $0\leq s_i \leq f_i \leq \inf$. $a_i$ and $a_j$ are 
    compatible if $s_i > f_j$ or $s_j > f_i$.

    In the activity-selection problem, we wish to select a maximum-size
    subset of mutually compatible activities.

    Assume that the activities are sorted in increasing order of finish time
    \begin{equation*}
        f_1 \leq f_2 \leq f_3 ... f_{n-1} \leq f_n
    \end{equation*}

    \includegraphics[width=0.4\textwidth]{contents/Advanced_Design/Greedy/greedy_image/sorted_finish_time.png}


    The approach is try to solve with dynamic programming and observe that 
    only one choice should be consider - the greedy choice, that is 
    DP with one subproblem.

    Let c[i,j] denoted the maximum size of an optimal solution for set $S_{ij}$.
    $S_{ij}=\lbrace a_k: f_i \leq s_k \leq f_k \leq s_j  \rbrace$.

    The reccurence of choosing $a_k$

    \begin{equation*}
        c[i,j] = c[i,k] + c[k,j] + 1
    \end{equation*}

    \[ c[i,j]= \begin{cases} 
        0 & \text{if } S_{ij}=\emptyset \\
        max_{a_k \in S_{ij}} \lbrace c[i,k]+c[k,j]+1 \rbrace &
        \text{if } S_{ij} \neq \emptyset
     \end{cases}
    \]
  
\subsubsection*{Making the greedy choice}


    Intuition suggests that we should choose an activity that leaves the resource available
    for as many other activities as possible: choose the earliest finish time.

    By choosing "the" activity, we only left one subproblem.
    
    Let $S_k = \lbrace a_i \in S: s_i \geq f_k \rbrace$ be the set of activities
    that start after $a_k$ finishes.

    But why is the intuition correct?

    \includegraphics[width=0.6\textwidth]{contents/Advanced_Design/Greedy/greedy_image/proof_earliest_finish_time.png}

    Greedy algorithms typically have this top-down design: make a
    choice and then solve a subproblem, 
    rather than the bottom-up technique of solving
    subproblems before making a choice.


\subsubsection*{A recursive greedy algorithm}

     Assume that the n input activities are already ordered 
     by monotonically increasing ﬁnish time.

     In order to start, we add the ﬁctitious 
     activity a0 with $f_0=0$, so that subproblem $S_0$ is
    the entire set of activities S.

    \begin{lstlisting}
        RECURSIVE-ACTIVITY-SELECTOR(s,f,k,n)
        {
            // select the earliest finish time activity
            int m = k+1;
            // such that s_m >= f_k (k is the last choice)
            while( m <= n && s[m] < f[k])
            {
                m = m+1;
            }

            if(m <= n)
            {
                // subproblem, maximum mutable compatible set of S_m
                return union(a_m,RECURSIVE-ACTIVITY-SELECTOR(s,f,m,n));
            }

            return []
        }
    \end{lstlisting}


    \textbf{Time Complexity:} O(n) which is very obvious.

\subsubsection*{An iterative greedy algorithm}

    \begin{lstlisting}
        GREEDY-ACTIVITY-SELECTOR(s,f)
        {
            int n = s.length;
            A = [a1];
            int k = 1;
            for(int m=1; m<n; m++)
            {
                if s[m] > f[k]
                {
                    A.append(a_m);
                    k = m;
                }
            }

            return A;
        }
    \end{lstlisting}



\subsection{Elements of the greedy strategy}

    The greedy strategy doesn't always yield an optimal solution.

    In order to use greedy method:
    \begin{enumerate}
        \item Determine the optimal substructure of the problem.
        \item Develop a recursive solution.
        \item Show that if we make the greedy choice, then only one subproblem
        remains.
        \item Prove that it is always safe to make the greedy choice.
        \item Develop a recursive algorithm that implements the greedy strategy.
        \item Convert the recursive algorithm to iterative algorithm.
    \end{enumerate}

    In the activity-selection problem, we proved that a greedy
    choice (the earliest finish time $a_m$ to finish in $S_k$), combined 
    with an optimal solution to the reaming $S_m$ of compatible activities,
    yields an optimal solution to $S_k$.
   
    More generally,
    \begin{enumerate}
        \item Think the optimization is to make a choice and solve the 
        remaining subproblem.
        \item Prove that there is always an optimal solution to the original
        problem that makes greedy choice $\Leftrightarrow$ the greedy
        choice is always safe.
        \item Demonstrate optimal substructure by showing that, having
        made the greedy choice, what remains is a subproblem with the property
        that if we combine an optimal solution to the subproblem with 
        the greedy choice, we arrive at an optimal solution to the original 
        problem.
    \end{enumerate}

    \subsubsection*{Greedy-choice property}

    A dynamic programming algorithm proceeds bottom up, whereas a greedy 
    strategy usually process in a top-down fashion.
    Of course, we must prove that a greedy choice at each step yields a globally
    optimal solution.

    \subsubsection*{Optimal substructure}

    A problem is said to have optimal substructure 
    if an optimal solution can be constructed from optimal solutions 
    of its subproblems.

    \subsubsection*{Dynamic Programming vs Greedy Algorithm}

    0/1 knapsack problem vs fractional knapsack problem.

    0/1 knapsack is actually a dynamic programming problem while 
    fractional knapsack can be solved greedily.

    \includegraphics[width=0.5\textwidth]{contents/Advanced_Design/Greedy/greedy_image/dp_vs_greedy.png}

    fractional knapsack can be solved by computing the value per pound and 
    ordering the value per pound in increasing order, and then apply 
    greedy strategy to attain the global optimal.

    However 0/1 knapsack cannot be solved by the above strategy, since 
    picking item 10 make introduce unusable space left, therefore, lowering 
    the actual value per pounds.


\subsection{Huffman Codes}

    Suppose there is 100,000-character data file that we wish 
    to store compactly. We count the frequencey of each word.

    \includegraphics[width=0.5\textwidth]{contents/Advanced_Design/Greedy/greedy_image/frequency_table.png}

    Here, we use \textbf{binary character code} in which each character
    is represented by a unique binary string, which we can
    call \textbf{codeword}.

    By considering the frequency of each character, a \textbf{variable-length code}
    can do better than a fixed-length code which requires 300,000 bits.

    From above table, the total number of bits that needed

    \begin{equation*}
        (45\cdot 1 + 13\cdot 3 + 12\cdot 3 +16\cdot 3 + 9\cdot 4 + 5\cdot 4) \cdot 1000 =  224,000 
    \end{equation*}

    \subsubsection*{Prefix codes}

    Note, in the above codewords. No codeword is a prefix of some other codeword.

    Prefix codes are desirable for decoding, chop each codeword then deal with 
    the remaining. It is unambiguous. For example, 00101101 would be 
    0-0-101-1101, that is \textbf{aabe}.
    
    We use binary tree to represent for prefix code, so that we can easily pick 
    off the initial codeword: "descend left" is 0, "descend right" is 1. An 
    optimal code for a file is always represented by a full binary tree that is 
    each nonleaf node has two children.

    \includegraphics[width=0.7\textwidth]{contents/Advanced_Design/Greedy/greedy_image/prefix_code_trees.png}

    If C is the alphabet, then the optimal prefix tree has |C| leaves. 1 leaf for 
    each letter, and exacly |C|-1 internal nodes. Note that the value of the trees,
    the tree is branched by the frequency of letters. 

    Given an optimal prefix tree, we can easily compute the nubmer of 
    required to encode a file. For each c in C, let c.freq denote the 
    frequency of c and $d_{T}(c)$ denote the depth of c's leaf in the tree.
    $d_{T}(c)$ is also the number of bits for the codeword of c.
    \begin{equation*}
        B(T) = \sum _{c\in C} c.freq\cdot d_{T}(c)
    \end{equation*}

\subsubsection*{Constructing a Huffman code}

    Huffman invented a greedy algorithm that constructs an optimal preﬁx code called
    a \textbf{Huffman code}.

    Assume that C is a set of n characters and that each character $c\in C$
    is an object with an attribute c.freq giving its frequency.

    \begin{lstlisting}
        HUFFMAN(C)
        {
            int n = |C|;
            
            /* Q is a min-priority queue, such that the key the frequency
                also, for each c in the C is already a node.
            */
            Q = C;

            for(int i=0; i<n-1;i++)
            {
                new node z;
                
                // extract the least two frequent node (letters)
                // the order is arbitrary
                x = EXTRACT-MIN(Q);
                y = EXTRACT-MIN(Q);
                z.left = x;
                z.right = y;

                // this is like merging two letters x,y into one "letter" z.
                z.freq = x.freq + y.freq;
                INSERT(Q.z);
            }

            // return the root
            return EXTRACT-MIN(Q);
        }
    \end{lstlisting}

    \textbf{Time complexity:} since Q is a priority queue,
    the EXTRACT-MIN() takes O(lgn), and the for loops execute
    exactly n-1 times. That is O(nlgn). By replacing the 
    priority queue with van Emde Boas tree, we can further reduce 
    the T(n) to O(nlglgn).

\subsubsection*{Correctness of Huffman's algorithm}

    To prove that the greedy algorithm H UFFMAN is correct, we show that the problem
    of determining an optimal preﬁx code exhibits the greedy-choice and optimal-
    substructure properties.

    





























